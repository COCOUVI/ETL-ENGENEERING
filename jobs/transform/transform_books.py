"""
PHASE ETL : TRANSFORM

Objectifs :
- Lire les donn√©es RAW depuis MinIO (JSON)
- Nettoyer et typer les donn√©es
- Appliquer des r√®gles de qualit√©
- √âcrire les donn√©es VALIDES en zone PROCESSED
- Logger les anomalies sans casser le pipeline
"""

import os
import logging
import boto3
from botocore.exceptions import ClientError
from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_replace, col, when, to_timestamp
from dotenv import load_dotenv

# ============================================================
# ENV & LOGGING
# ============================================================

load_dotenv()

MINIO_ACCESS_KEY = os.getenv("MINIO_ROOT_USER")
MINIO_SECRET_KEY = os.getenv("MINIO_ROOT_PASSWORD")
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9001")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# ============================================================
# MINIO / S3 HELPERS
# ============================================================

def ensure_bucket_exists(bucket_name: str):
    """
    V√©rifie l'existence d'un bucket MinIO, le cr√©e si n√©cessaire
    """
    s3_client = boto3.client(
        "s3",
        endpoint_url=MINIO_ENDPOINT,
        aws_access_key_id=MINIO_ACCESS_KEY,
        aws_secret_access_key=MINIO_SECRET_KEY,
        region_name="us-east-1"
    )

    try:
        s3_client.head_bucket(Bucket=bucket_name)
        logging.info(f"Bucket '{bucket_name}' already exists ‚úÖ")
    except ClientError:
        logging.info(f"Bucket '{bucket_name}' not found ‚Üí creating üöÄ")
        s3_client.create_bucket(Bucket=bucket_name)
        logging.info(f"Bucket '{bucket_name}' created successfully ‚úÖ")


def s3_folder_exists(s3_client, bucket, prefix):
    """
    V√©rifie si un dossier (prefix) contient d√©j√† des objets
    """
    resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
    return "Contents" in resp and len(resp["Contents"]) > 0

# ============================================================
# SPARK
# ============================================================

def create_spark_session() -> SparkSession:
    """
    Cr√©e une SparkSession configur√©e pour MinIO (S3A)
    """
    logging.info("Initializing Spark session")

    jar_path = "/opt/airflow/hadoop-aws-3.2.4.jar,/opt/airflow/aws-java-sdk-bundle-1.11.1026.jar"

    return (
        SparkSession.builder
        .appName("TransformBooks")
        .config("spark.jars", jar_path)
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT)
        .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY)
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY)
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        .config("spark.sql.shuffle.partitions", "4")
        .getOrCreate()
    )

# ============================================================
# ETL ‚Äì TRANSFORM
# ============================================================

def read_raw_books(spark: SparkSession):
    """
    Lecture des fichiers RAW depuis MinIO
    """
    path = "s3a://raw/books/books_*.json"
    logging.info(f"Reading RAW data from {path}")

    return (
        spark.read
        .option("multiline", "true")
        .json(path)
    )


def clean_books(df_raw):
    """
    Nettoyage et typage des donn√©es
    """
    logging.info("Cleaning books data")

    df = df_raw.withColumn(
        "price",
        regexp_replace(col("price"), "[^0-9.]", "").cast("double")
    )

    df = df.withColumn(
        "rating",
        when(col("rating") == "One", 1)
        .when(col("rating") == "Two", 2)
        .when(col("rating") == "Three", 3)
        .when(col("rating") == "Four", 4)
        .when(col("rating") == "Five", 5)
        .otherwise(None)
    )

    df = df.withColumn(
        "scraped_at",
        to_timestamp(col("scraped_at"))
    )

    return df


def apply_data_quality_checks(df):
    """
    Applique les r√®gles de qualit√© :
    - title non null
    - price > 0
    - rating entre 1 et 5
    - scraped_at non null
    """
    logging.info("Applying data quality rules")

    valid_df = df.filter(
        (col("title").isNotNull()) &
        (col("price").isNotNull()) &
        (col("price") > 0) &
        (col("rating").between(1, 5)) &
        (col("scraped_at").isNotNull())
    )

    invalid_df = df.subtract(valid_df)

    logging.info(f"Valid records  : {valid_df.count()}")
    logging.info(f"Invalid records: {invalid_df.count()}")

    return valid_df, invalid_df


def write_processed_books(df_valid):
    """
    √âcriture des donn√©es VALIDES en zone PROCESSED
    (MinIO + local)
    """
    s3_output = "s3a://processed/books/"
    local_output = "data/processed/books/"

    ensure_bucket_exists("processed")

    s3_client = boto3.client(
        "s3",
        endpoint_url=MINIO_ENDPOINT,
        aws_access_key_id=MINIO_ACCESS_KEY,
        aws_secret_access_key=MINIO_SECRET_KEY,
        region_name="us-east-1"
    )

    # --- MinIO ---
    if not s3_folder_exists(s3_client, "processed", "books/"):
        logging.info(f"Writing processed data to {s3_output}")
        df_valid.write.mode("overwrite").parquet(s3_output)
    else:
        logging.info("[SKIP] Processed data already exists in MinIO")

    # --- Local ---
    if not os.path.exists(local_output) or not os.listdir(local_output):
        logging.info(f"Writing processed data locally to {local_output}")
        os.makedirs(local_output, exist_ok=True)
        df_valid.write.mode("overwrite").parquet(local_output)
    else:
        logging.info("[SKIP] Processed data already exists locally")

# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":

    spark = create_spark_session()

    # 1Ô∏è‚É£ READ
    df_raw = read_raw_books(spark)
    df_raw.show(5, truncate=False)

    # 2Ô∏è‚É£ CLEAN
    df_clean = clean_books(df_raw)
    df_clean.show(5, truncate=False)

    # 3Ô∏è‚É£ QUALITY
    df_valid, df_invalid = apply_data_quality_checks(df_clean)

    if df_invalid.count() > 0:
        logging.warning("Some invalid records were detected")
        df_invalid.show(5, truncate=False)

    # 4Ô∏è‚É£ WRITE
    write_processed_books(df_valid)

    # 5Ô∏è‚É£ SCHEMA
    df_valid.printSchema()

    spark.stop()
